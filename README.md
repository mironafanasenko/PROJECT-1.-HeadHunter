# Проект 1. Анализ резюме из HeadHunter 
<table>
  <tr style="vertical-align:middle">
    <th><img src = 'https://i.hh.ru/logos/svg/hh.ru__min_.svg?v=11032019'></th>
    <!-- <th><img style="vertical-align:middle" img src = https://lms.skillfactory.ru/static/rg-theme/images/logo-header.svg></th> -->
    <!-- <th><img style="vertical-align:middle" img src = https://static.tildacdn.com/tild3862-3932-4061-b763-363135393134/logo.svg></th> -->
    <th height=30><img style="vertical-align:middle" img src = https://static.tildacdn.com/tild3736-6663-4331-b065-623334663336/SkillFactory.svg height=20></th>
  </tr>
</table>
## Описание проекта
На основе базы резюме, выгруженной с сайта поиска вакансий HH.ru, необходимо подготовить первичные данные для построения модели, которая бы автоматически определяла примерный уровень заработной платы, подходящей пользователю, исходя из информации, которую он указал о себе. 

<br>

### Какой кейс решаем?
Нужно предварительно подготовить имеющиеся данные путем их изучения, преобразования с возможностью дальнейшего исследования и проведения очистки, что позволит в дальнейшем построить математическую прогнозную модель.

<br>

**Условия:**
- Каждая часть проекта представляет собой блок практических заданий, которые небходимо выполнить в шаблоне jupyter-ноутбука от SkillFactory;
- При выполнении проекта также необходимо ответить на контрольные вопросы на платформе;
- Каждое задание выполняется в отдельной ячейке, выделенной под задание, код для каждого задания оформляется в одной-двух jupyter-ячейках;
- Код должен быть читаемым и понятным: имена переменных и функций должны отражать их сущность, важно избегать многострочных конструкций и условий;
- Графики оформляются в соответствии с теми правилами, которые изучались в модуле по визуализации данных. Обязательное требование: графики должны содержать название, отражающее их суть, и подписи осей;
- Выводы к графикам оформляются в формате ***Markdown*** под самим графиком в отдельной ячейке.

<br>

**Метрика качества:**
* Результаты оцениваются согласно требованиям, указанным к проекту. 
* Необходимо: ответить на контрольные вопросы (максимум 30 баллов), сдать проект на проверку, загрузив ноутбук-шаблон со своим решением на GitHub (максимум 10 баллов).

<br>

**Что на практике:**
-   Учусь писать отличный код на Python;
-   Учусь эффективно работать с IDE VSCode;
-   Повышаю квалификацию по методам преобразования и очистки данных; 
-   Повышаю квалификацию с GitHub.


<br>

### Краткая информация о данных
В данном проекте первоначальные данные представлены в виде датасета размером: 44744 строки, 12 столбцов, типа object, в отдельных столбцах присутствуют пропуски и дубликаты. В связи с этим необходимо более детально проанализировать структуру первоначальных данных, сделать выводы о дальнейших преобразованиях. 

-  dst-3.0_16_1_hh_database.csv - база резюме, выгруженная с сайта поиска вакансий hh.ru, предварительно подготовленная SkillFactory;
-  ExchangeRates.csv - курсы валют, предоставленные SkillFactory.

<br>

### Требования для работы
*   Основой интерпретатор - Python 3.10 (у меня взят из Windows App Store для максимальной бесшовной интеграции с VSCode);
*   Дополнительные требования перечислены в requirements.txt (получены командой pip freeze > requirements.txt);
*   Установка всех недостающих дополнительных компонент:

            pip install -r requirements.txt

*   В проекте используется модуль <font color='LightSeaGreen'>**Plotly.io**</font> для сохранения диаграмм и графиков в файл, для сохранения доступны 2 "движка", предварительно нужно установить любой, на ваш выбор, чтобы механизм экспорта на диск работал:
    * "kaleido",

          pip install kaleido

    или, как в моём проекте, я использовал

    * "orca"

          pip install orca

    Документация по использованию [Orca](https://github.com/plotly/orca) на GitHub.

    Документация по использованию [Plotly.io](https://plotly.github.io/plotly.py-docs/generated/plotly.io.write_image.html)

*   Для выгрузки исходного датасета, применил надстройку GitHub - [Git LFS](https://git-lfs.github.com/). 

    Документация по использованию Git LFS - [Docs Git LFS](https://github.com/git-lfs/git-lfs/tree/main/docs)


<br>

### Этапы работы над проектом
* Ознакомление с описанием задачи;
* Базовый анализ исходных данных;
* Преобразование данных;
* Разведывательный анализ;
* Очистка данных;
* Проверка соответствия результата выполнения кода условию, приведенному в метрике качества;
* Проверка соответствия написанного кода стандарту PEP8;
* Оформление проекта;
* Загрузка проекта на GitHub. 

<br>


<br>

### Выводы
В процессе выполнения кейса первоначальные данные были:
* Проанализированы;
* Преобразованы в соответствии с поставленным ТЗ;
* Проведен разведывательный анализ данных с последующей визуализацией результатов для выявления взаимосвязей между признаками;
* Данные были очищены от пропусков и дубликатов. 

Таким образом, первоначальный датасет подготовлен для дальнейшего использования при построении требуемой модели.
